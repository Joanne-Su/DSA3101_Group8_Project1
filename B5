import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

dfTrain = pd.read_csv("/content/sample_data/TrainingData.csv")
dfValid = pd.read_csv("/content/sample_data/ValidationData.csv")
df = pd.concat([dfTrain, dfValid])
import geopandas as gpd
from shapely.geometry import Point

# Convert dataframe to GeoDataFrame
df["geometry"] = df.apply(lambda row: Point(row["LONGITUDE"], row["LATITUDE"]), axis=1)
gdf = gpd.GeoDataFrame(df, geometry="geometry")

# Plot using GeoPandas
gdf.plot(marker="o", color="red", markersize=5)
plt.show() # Shows the outline of the buildings.

# To identify the number of distinct users and phones.
print(f"There are {df['USERID'].nunique()} distinct users.\n")
print(f"There are {df['PHONEID'].nunique()} different phones used.\n")

df.groupby('USERID')['PHONEID'].unique()


# Distribution of access points.
pivot_df = dfgrouped.pivot(index="FLOOR", columns="BUILDINGID", values="num_access_points")

# Plot the bar chart
pivot_df.plot(kind="bar", figsize=(10, 6))

# Add labels and title
plt.xlabel("Floor")
plt.ylabel("Number of Access Points")
plt.title("Access Points per Floor for Each Building")
plt.legend(title="Building ID")

plt.show()


# Count unique spaces on each floor.
df_filtered = df[df["BUILDINGID"] == 0]

# Group by FLOOR and count unique SPACEID values
unique_spaceid_counts = df_filtered.groupby("FLOOR")["SPACEID"].nunique()

# Display result
print(f'Building 0:\n{unique_spaceid_counts}\n')

df_filtered = df[df["BUILDINGID"] == 1]

unique_spaceid_counts = df_filtered.groupby("FLOOR")["SPACEID"].nunique()

print(f'Building 1:\n{unique_spaceid_counts}\n')

df_filtered = df[df["BUILDINGID"] == 2]

unique_spaceid_counts = df_filtered.groupby("FLOOR")["SPACEID"].nunique()

print(f'Building 2:\n{unique_spaceid_counts}\n')


# Convert epoch time to unix time yyyy-mm-dd HH:MM:SS
df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], unit='s')
df['DATE'] = df['TIMESTAMP'].dt.date      # Extracts YYYY-MM-DD
df['TIME'] = df['TIMESTAMP'].dt.time  

# Number of users in each building per day.
df_grouped = df.groupby(['DATE', 'BUILDINGID'])['USERID'].nunique().reset_index()
# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=df_grouped, x='DATE', y='USERID', hue='BUILDINGID', dodge=True)

# Labels
plt.xlabel("Date")
plt.ylabel("Number of Users")
plt.title("Number of Users in Each Building per Day")
plt.xticks(rotation=45)
plt.legend(title="Building ID")

# Show plot
plt.show()




# ================================================================ Neural Network Regression Model ================================================================

# Remove weak signals 
def filter_weak_signals(df):
    wap_columns = [col for col in df.columns if col.startswith("WAP")]
    df[wap_columns] = df[wap_columns].applymap(lambda x: x if x > -57 else 100)
    return df

dfTrain = filter_weak_signals(dfTrain)
dfValid = filter_weak_signals(dfValid)

# Load & preprocess data
train_x = dfTrain.drop(columns=["LONGITUDE", "LATITUDE", "FLOOR", "BUILDINGID", "SPACEID", "RELATIVEPOSITION", "USERID", "PHONEID", "TIMESTAMP"])
train_y = dfTrain[["LONGITUDE", "LATITUDE"]]

test_x = dfValid.drop(columns=["LONGITUDE", "LATITUDE", "FLOOR", "BUILDINGID", "SPACEID", "RELATIVEPOSITION", "USERID", "PHONEID", "TIMESTAMP"])
test_y = dfValid[["LONGITUDE", "LATITUDE"]]

# Feature Scaling
scaler = StandardScaler()
train_x_scaled = scaler.fit_transform(train_x)
test_x_scaled = scaler.transform(test_x)

# Build the model with Dropout & L2 Regularization
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(train_x_scaled.shape[1],)),
    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(2, activation='linear')  # Output for longitude and latitude
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mean_absolute_error',
    metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.CosineSimilarity(axis=1)]
)

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Train model
epochs = 50
history = model.fit(
    train_x_scaled,
    train_y.values.astype('float'),
    validation_data=(test_x_scaled, test_y),
    epochs=epochs,
    batch_size=32,
    callbacks=[early_stop, lr_scheduler]
)

# Evaluate model
eval_result = model.evaluate(test_x_scaled, test_y)
print(f"Test loss (MAE): {eval_result[0]}")
print(f"Test RMSE: {eval_result[1]}")

# Plot Training Progress
plt.figure(figsize=(10, 5))
plt.plot(history.history['val_root_mean_squared_error'], label="Validation RMSE")
plt.plot(history.history['root_mean_squared_error'], label="Training RMSE")
plt.xlabel("Epoch")
plt.ylabel("RMSE")
plt.legend()
plt.show()

# Predictions
pred_y = model.predict(test_x_scaled)

# Scatter Plot: True vs. Predicted
plt.figure(figsize=(10, 5))
plt.scatter(test_y["LONGITUDE"], test_y["LATITUDE"], label="Actual", alpha=0.5)
plt.scatter(pred_y[:, 0], pred_y[:, 1], label="Predicted", alpha=0.5)
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.legend()
plt.title("Predicted vs. True Locations")
plt.show()

# Error Histogram
errors = np.sqrt((pred_y[:, 0] - test_y["LONGITUDE"])**2 + (pred_y[:, 1] - test_y["LATITUDE"])**2)
sns.histplot(errors, bins=30, kde=True)
plt.xlabel("Prediction Error (meters)")
plt.ylabel("Frequency")
plt.title("Error Distribution")
plt.show()

# Residual Plot
residuals = pred_y - test_y.values
plt.figure(figsize=(10, 5))
plt.scatter(test_y.index, residuals[:, 0], label="Longitude Error", alpha=0.5)
plt.scatter(test_y.index, residuals[:, 1], label="Latitude Error", alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Index")
plt.ylabel("Error")
plt.legend()
plt.title("Residuals Plot")
plt.show()

# Geographical Heatmap (if floor plan available)
plt.figure(figsize=(10, 5))
sns.kdeplot(x=pred_y[:, 0], y=pred_y[:, 1], cmap="coolwarm", fill=True)
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Prediction Density Heatmap")
plt.show()

# Output
624/624 ━━━━━━━━━━━━━━━━━━━━ 8s 13ms/step - cosine_similarity: 1.0000 - loss: 151688.6406 - root_mean_squared_error: 284697.4688 - val_cosine_similarity: 1.0000 - val_loss: 61278.2578 - val_root_mean_squared_error: 595415.2500 - learning_rate: 6.2500e-05
35/35 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - cosine_similarity: 1.0000 - loss: 38562.6836 - root_mean_squared_error: 286999.5625
Test loss (MAE): 52292.3359375
Test RMSE: 584455.8125


# ================================================================ Updated Neural Network Regression Model ================================================================

# Remove weak signals 
def filter_weak_signals(df):
    wap_columns = [col for col in df.columns if col.startswith("WAP")]
    df[wap_columns] = df[wap_columns].applymap(lambda x: x if x > -57 else 100)
    return df

dfTrain = filter_weak_signals(dfTrain)
dfValid = filter_weak_signals(dfValid)

# Add unique user count as a feature
dfTrain['unique_user_count'] = dfTrain.groupby(['LONGITUDE', 'LATITUDE'])['USERID'].transform('nunique')
dfValid['unique_user_count'] = dfValid.groupby(['LONGITUDE', 'LATITUDE'])['USERID'].transform('nunique')

train_x = dfTrain.drop(columns=["LONGITUDE", "LATITUDE", "FLOOR", "BUILDINGID", "SPACEID", "RELATIVEPOSITION", "USERID", "PHONEID", "TIMESTAMP"])
train_y = dfTrain['unique_user_count']

test_x = dfValid.drop(columns=["LONGITUDE", "LATITUDE", "FLOOR", "BUILDINGID", "SPACEID", "RELATIVEPOSITION", "USERID", "PHONEID", "TIMESTAMP"])
test_y = dfValid['unique_user_count']

scaler = StandardScaler()
train_x_scaled = scaler.fit_transform(train_x)
test_x_scaled = scaler.transform(test_x)

# Build the model with Dropout & L2 Regularization
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(train_x_scaled.shape[1],)),
    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(1, activation='linear')  # Output for unique user count
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mean_absolute_error',
    metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()]
)

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Train model
epochs = 50
history = model.fit(
    train_x_scaled,
    train_y.values.astype('float'),
    validation_data=(test_x_scaled, test_y),
    epochs=epochs,
    batch_size=32,
    callbacks=[early_stop, lr_scheduler]
)

# Evaluate model
eval_result = model.evaluate(test_x_scaled, test_y)
print(f"Test loss (MAE): {eval_result[0]}")
print(f"Test RMSE: {eval_result[1]}")

# Plot Training Progress
plt.figure(figsize=(10, 5))
plt.plot(history.history['val_root_mean_squared_error'], label="Validation RMSE")
plt.plot(history.history['root_mean_squared_error'], label="Training RMSE")
plt.xlabel("Epoch")
plt.ylabel("RMSE")
plt.legend()
plt.show()

# Predictions
pred_y = model.predict(test_x_scaled)

# Scatter Plot: True vs. Predicted
plt.figure(figsize=(10, 5))
plt.scatter(test_y, pred_y, label="Predicted", alpha=0.5)
plt.plot([test_y.min(), test_y.max()], [test_y.min(), test_y.max()], 'r--', lw=2, label="Ideal")
plt.xlabel("True Unique User Count")
plt.ylabel("Predicted Unique User Count")
plt.legend()
plt.title("Predicted vs. True Unique User Counts")
plt.show()

# Error Histogram
errors = pred_y.flatten() - test_y.values
sns.histplot(errors, bins=30, kde=True)
plt.xlabel("Prediction Error (Unique User Count)")
plt.ylabel("Frequency")
plt.title("Error Distribution")
plt.show()

# Residual Plot
residuals = pred_y.flatten() - test_y.values
plt.figure(figsize=(10, 5))
plt.scatter(test_y.index, residuals, label="Residuals", alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Index")
plt.ylabel("Residuals (Prediction Error)")
plt.legend()
plt.title("Residuals Plot")
plt.show()

# ================================================== OUTPUT ==================================================
Epoch 45/50
624/624 ━━━━━━━━━━━━━━━━━━━━ 7s 12ms/step - loss: 0.1676 - mean_absolute_error: 0.1388 - root_mean_squared_error: 0.2403 - val_loss: 0.2693 - val_mean_absolute_error: 0.2407 - val_root_mean_squared_error: 0.4614 - learning_rate: 7.8125e-06
35/35 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.2508 - mean_absolute_error: 0.2187 - root_mean_squared_error: 0.4136
Test loss (MAE): 0.2388780564069748
Test RMSE: 0.4047054648399353




# ================================================================ LSTM Model for Crowd Analysis (Time Series) ================================================================

# Add unique user count as a feature
dfTrain['unique_user_count'] = dfTrain.groupby(['LONGITUDE', 'LATITUDE'])['USERID'].transform('nunique')
dfValid['unique_user_count'] = dfValid.groupby(['LONGITUDE', 'LATITUDE'])['USERID'].transform('nunique')

dfTrain['TIMESTAMP'] = pd.to_datetime(dfTrain['TIMESTAMP'], unit='s')

dfValid['TIMESTAMP'] = pd.to_datetime(dfValid['TIMESTAMP'], unit='s')


dfTrain['hour'] = dfTrain['TIMESTAMP'].dt.hour
dfTrain['day_of_week'] = dfTrain['TIMESTAMP'].dt.dayofweek
dfTrain['month'] = dfTrain['TIMESTAMP'].dt.month

dfValid['hour'] = dfValid['TIMESTAMP'].dt.hour
dfValid['day_of_week'] = dfValid['TIMESTAMP'].dt.dayofweek
dfValid['month'] = dfValid['TIMESTAMP'].dt.month

# Prepare sequences for LSTM
def create_sequences(data, target, sequence_length):
    sequences = []
    targets = []
    for i in range(len(data) - sequence_length):
        sequences.append(data[i:i+sequence_length])
        targets.append(target[i+sequence_length])
    return np.array(sequences), np.array(targets)

sequence_length = 10  # Example sequence length
features = ['unique_user_count', 'hour']  # Add other relevant features

train_x, train_y = create_sequences(dfTrain[features].values, dfTrain['unique_user_count'].values, sequence_length)
test_x, test_y = create_sequences(dfValid[features].values, dfValid['unique_user_count'].values, sequence_length)

# Build the LSTM model
model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(128, activation='relu', input_shape=(train_x.shape[1], train_x.shape[2])),
    tf.keras.layers.Dense(1)
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mean_absolute_error',
    metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()]
)

# Train model
epochs = 50
history = model.fit(
    train_x,
    train_y,
    validation_data=(test_x, test_y),
    epochs=epochs,
    batch_size=32,
    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)]
)

# Evaluate model
eval_result = model.evaluate(test_x, test_y)
print(f"Test loss (MAE): {eval_result[0]}")
print(f"Test RMSE: {eval_result[1]}")

# Predictions
pred_y = model.predict(test_x)

# Scatter Plot: True vs. Predicted
plt.figure(figsize=(10, 5))
plt.scatter(test_y, pred_y, label="Predicted", alpha=0.5)
plt.plot([test_y.min(), test_y.max()], [test_y.min(), test_y.max()], 'r--', lw=2, label="Ideal")
plt.xlabel("True Unique User Count")
plt.ylabel("Predicted Unique User Count")
plt.legend()
plt.title("Predicted vs. True Unique User Counts")
plt.show()

# Error Histogram
errors = pred_y.flatten() - test_y
sns.histplot(errors, bins=30, kde=True)
plt.xlabel("Prediction Error (Unique User Count)")
plt.ylabel("Frequency")
plt.title("Error Distribution")
plt.show()

# Residual Plot
residuals = pred_y.flatten() - test_y
plt.figure(figsize=(10, 5))
plt.scatter(range(len(residuals)), residuals, label="Residuals", alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Index")
plt.ylabel("Residuals (Prediction Error)")
plt.legend()
plt.title("Residuals Plot")
plt.show()


# ================================================== OUTPUT ==================================================
Epoch 16/50
623/623 ━━━━━━━━━━━━━━━━━━━━ 11s 16ms/step - loss: 0.7881 - mean_absolute_error: 0.7881 - root_mean_squared_error: 1.4875 - val_loss: 0.1978 - val_mean_absolute_error: 0.1978 - val_root_mean_squared_error: 0.3831
35/35 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.1243 - mean_absolute_error: 0.1243 - root_mean_squared_error: 0.2451
Test loss (MAE): 0.10889586061239243
Test RMSE: 0.23518501222133636


# ================================================================ Generated Future Dates to test model ================================================================

future_dates = pd.date_range(start='2025-03-22', periods=30, freq='h')  # Example: next 30 hours

future_df = pd.DataFrame({'DATETIME': future_dates})

future_df['hour'] = future_df['DATETIME'].dt.hour
future_df['day_of_week'] = future_df['DATETIME'].dt.dayofweek
future_df['month'] = future_df['DATETIME'].dt.month

future_df['unique_user_count'] = np.random.randint(50, 100, size=len(future_df))  # Replace with actual estimation method

last_known_data = dfTrain[['unique_user_count', 'hour']].values[-sequence_length:]

def create_future_sequences(last_data, future_data, sequence_length):
    sequences = []
    for i in range(len(future_data)):
        sequence = np.vstack([last_data, future_data[:i+1]])
        if sequence.shape[0] > sequence_length:
            sequence = sequence[-sequence_length:]
        sequences.append(sequence)
    return np.array(sequences)

sequence_length = 10  # Example sequence length
features = ['unique_user_count', 'hour']  # Use only the relevant features

future_x = create_future_sequences(last_known_data, future_df[features].values, sequence_length)

# Ensure the input shape matches the model's expected input shape
print("Shape of future_x:", future_x.shape)
print("Expected input shape:", model.input_shape)

future_x = future_x.reshape((future_x.shape[0], sequence_length, len(features)))

# Make predictions
try:
    future_predictions = model.predict(future_x)
    print("Future Predictions:", future_predictions)
except tf.errors.InvalidArgumentError as e:
    print(f"InvalidArgumentError: {e}")
