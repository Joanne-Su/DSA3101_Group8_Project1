import pandas as pd
import matplotlib.pyplot as plt

dfTrain = pd.read_csv("/content/sample_data/TrainingData.csv")
dfValid = pd.read_csv("/content/sample_data/ValidationData.csv")
df = pd.concat([dfTrain, dfValid])
import geopandas as gpd
from shapely.geometry import Point

# Convert dataframe to GeoDataFrame
df["geometry"] = df.apply(lambda row: Point(row["LONGITUDE"], row["LATITUDE"]), axis=1)
gdf = gpd.GeoDataFrame(df, geometry="geometry")

# Plot using GeoPandas
gdf.plot(marker="o", color="red", markersize=5)
plt.show() # Shows the outline of the buildings.

# To identify the number of distinct users and phones.
print(f"There are {df['USERID'].nunique()} distinct users.\n")
print(f"There are {df['PHONEID'].nunique()} different phones used.\n")

df.groupby('USERID')['PHONEID'].unique()


# Distribution of access points.
pivot_df = dfgrouped.pivot(index="FLOOR", columns="BUILDINGID", values="num_access_points")

# Plot the bar chart
pivot_df.plot(kind="bar", figsize=(10, 6))

# Add labels and title
plt.xlabel("Floor")
plt.ylabel("Number of Access Points")
plt.title("Access Points per Floor for Each Building")
plt.legend(title="Building ID")

plt.show()


# Count unique spaces on each floor.
df_filtered = df[df["BUILDINGID"] == 0]

# Group by FLOOR and count unique SPACEID values
unique_spaceid_counts = df_filtered.groupby("FLOOR")["SPACEID"].nunique()

# Display result
print(f'Building 0:\n{unique_spaceid_counts}\n')

df_filtered = df[df["BUILDINGID"] == 1]

unique_spaceid_counts = df_filtered.groupby("FLOOR")["SPACEID"].nunique()

print(f'Building 1:\n{unique_spaceid_counts}\n')

df_filtered = df[df["BUILDINGID"] == 2]

unique_spaceid_counts = df_filtered.groupby("FLOOR")["SPACEID"].nunique()

print(f'Building 2:\n{unique_spaceid_counts}\n')


# Convert epoch time to unix time yyyy-mm-dd HH:MM:SS
df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], unit='s')
df['DATE'] = df['TIMESTAMP'].dt.date      # Extracts YYYY-MM-DD
df['TIME'] = df['TIMESTAMP'].dt.time  

# Number of users in each building per day.
df_grouped = df.groupby(['DATE', 'BUILDINGID'])['USERID'].nunique().reset_index()
# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=df_grouped, x='DATE', y='USERID', hue='BUILDINGID', dodge=True)

# Labels
plt.xlabel("Date")
plt.ylabel("Number of Users")
plt.title("Number of Users in Each Building per Day")
plt.xticks(rotation=45)
plt.legend(title="Building ID")

# Show plot
plt.show()




# ================================================================ Crowd Analysis using Neural Network Regression Model ================================================================
import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Remove weak signals (WAPs with values below -90 dBm)
def filter_weak_signals(df):
    wap_columns = [col for col in df.columns if col.startswith("WAP")]
    df[wap_columns] = df[wap_columns].applymap(lambda x: x if x > -75 else 100)
    return df

dfTrain = filter_weak_signals(dfTrain)
dfValid = filter_weak_signals(dfValid)

# Load & preprocess data
train_x = dfTrain.drop(columns=["LONGITUDE", "LATITUDE", "FLOOR", "BUILDINGID", "SPACEID", "RELATIVEPOSITION", "USERID", "PHONEID", "TIMESTAMP"])
train_y = dfTrain[["LONGITUDE", "LATITUDE"]]

test_x = dfValid.drop(columns=["LONGITUDE", "LATITUDE", "FLOOR", "BUILDINGID", "SPACEID", "RELATIVEPOSITION", "USERID", "PHONEID", "TIMESTAMP"])
test_y = dfValid[["LONGITUDE", "LATITUDE"]]

# Feature Scaling
scaler = StandardScaler()
train_x_scaled = scaler.fit_transform(train_x)
test_x_scaled = scaler.transform(test_x)

# Build the model with Dropout & L2 Regularization
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(train_x_scaled.shape[1],)),
    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(2, activation='linear')  # Output for longitude and latitude
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mean_absolute_error',
    metrics=[tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.CosineSimilarity(axis=1)]
)

# Callbacks: Early Stopping & Learning Rate Scheduler
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Train model
epochs = 50
result = model.fit(
    train_x_scaled,
    train_y.values.astype('float'),
    validation_data=(test_x_scaled, test_y),
    epochs=epochs,
    batch_size=32,
    callbacks=[early_stop, lr_scheduler]
)

# Evaluate model
eval_result = model.evaluate(test_x_scaled, test_y)
print(f"Test loss (MAE): {eval_result[0]}")
print(f"Test RMSE: {eval_result[1]}")

# Plot Training Progress
plt.figure(figsize=(10, 5))
plt.plot(history.history['val_root_mean_squared_error'], label="Validation RMSE")
plt.plot(history.history['root_mean_squared_error'], label="Training RMSE")
plt.xlabel("Epoch")
plt.ylabel("RMSE")
plt.legend()
plt.show()

# Predictions
pred_y = model.predict(test_x_scaled)

# Scatter Plot: True vs. Predicted
plt.figure(figsize=(10, 5))
plt.scatter(test_y["LONGITUDE"], test_y["LATITUDE"], label="Actual", alpha=0.5)
plt.scatter(pred_y[:, 0], pred_y[:, 1], label="Predicted", alpha=0.5)
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.legend()
plt.title("Predicted vs. True Locations")
plt.show()

# Error Histogram
errors = np.sqrt((pred_y[:, 0] - test_y["LONGITUDE"])**2 + (pred_y[:, 1] - test_y["LATITUDE"])**2)
sns.histplot(errors, bins=30, kde=True)
plt.xlabel("Prediction Error (meters)")
plt.ylabel("Frequency")
plt.title("Error Distribution")
plt.show()

# Residual Plot
residuals = pred_y - test_y.values
plt.figure(figsize=(10, 5))
plt.scatter(test_y.index, residuals[:, 0], label="Longitude Error", alpha=0.5)
plt.scatter(test_y.index, residuals[:, 1], label="Latitude Error", alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Index")
plt.ylabel("Error")
plt.legend()
plt.title("Residuals Plot")
plt.show()

# Geographical Heatmap (if floor plan available)
plt.figure(figsize=(10, 5))
sns.kdeplot(x=pred_y[:, 0], y=pred_y[:, 1], cmap="coolwarm", fill=True)
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Prediction Density Heatmap")
plt.show()

# Output
624/624 ━━━━━━━━━━━━━━━━━━━━ 8s 13ms/step - cosine_similarity: 1.0000 - loss: 151688.6406 - root_mean_squared_error: 284697.4688 - val_cosine_similarity: 1.0000 - val_loss: 61278.2578 - val_root_mean_squared_error: 595415.2500 - learning_rate: 6.2500e-05
35/35 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - cosine_similarity: 1.0000 - loss: 38562.6836 - root_mean_squared_error: 286999.5625
Test loss (MAE): 52292.3359375
Test RMSE: 584455.8125
